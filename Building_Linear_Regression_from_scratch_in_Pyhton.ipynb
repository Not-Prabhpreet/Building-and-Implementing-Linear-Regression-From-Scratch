{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIXAoSEtcV2X"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression"
      ],
      "metadata": {
        "id": "YH47REb2C6l0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear_Regression():\n",
        "\n",
        "#initiating the parameters(learning rate and the number of iterations) these are hyperparameters and we need to give values for them manually that is why we are passing it in  a constructor so that every instance of the class has these two properties defined for it on creation\n",
        "\n",
        "  def __init__(self,learning_rate, no_of_iterations):\n",
        "    self.learning_rate= learning_rate\n",
        "    self.no_of_iterations= no_of_iterations\n",
        "\n",
        "  def fit(self,X,Y):\n",
        "    #Number of training examples(m) and number of features(n)\n",
        "    self.m, self.n=X.shape\n",
        "    #Initiating the weight and bias of our model that is randomly selecting selecting the weight and the bias initially\n",
        "    self.w=np.zeros(self.n) #We are creating a numpy array for weights as different features will have different weights\n",
        "    self.b= 0 #We are not creating an array for bias because the bias is always the same for all the features\n",
        "\n",
        "    self.X=X\n",
        "    self.Y=Y\n",
        "    #implementing gradient descent\n",
        "    for i in range(self.number_of_iterations):\n",
        "      self.update_weights()\n",
        "\n",
        "  def update_weights(self):\n",
        "    Y_prediction= self.predict(self.X)\n",
        "    #calculate gradients\n",
        "    dw= -(2*(self.X.T).dot(self.Y-Y_prediction))/self.m\n",
        "\n",
        "    db= -2* np.sum(self.Y- Y_prediction)/self.m\n",
        "\n",
        "    #Updating weights and bias\n",
        "    self.w= self.w - self.learning_rate*dw\n",
        "    self.b= self.b - self.learning_rate*db\n",
        "\n",
        "\n",
        "  def predict(self):\n",
        "    return X.dot(self.w)+ self.b\n"
      ],
      "metadata": {
        "id": "qLz5u6WhC-w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ScWzs25IC1Uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ja85bAPPJLHD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}